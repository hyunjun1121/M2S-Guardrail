#!/usr/bin/env python3
"""
M2S-Guardrail ì¢…í•© ì‹¤í—˜ ëŸ¬ë„ˆ
RTX A5000 8-GPU í™˜ê²½ì—ì„œ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ QLoRAë¡œ í›ˆë ¨
"""

import os
import json
import time
import torch
import pandas as pd
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from datasets import Dataset, concatenate_datasets
import multiprocessing as mp
from pathlib import Path

class ExperimentRunner:
    """ì‹¤í—˜ ì‹¤í–‰ ê´€ë¦¬ì"""
    
    def __init__(self, base_output_dir="./experiments"):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(exist_ok=True)
        
        # ì‹¤í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ì •ì˜
        self.experiment_matrix = self.define_experiment_matrix()
        
    def define_experiment_matrix(self):
        """ëª¨ë“  ì‹¤í—˜ ê²½ìš°ì˜ ìˆ˜ ì •ì˜"""
        
        # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ë¡œì»¬ ê²½ë¡œ ìš°ì„ , HuggingFace Hub ë°±ì—…)
        models = [
            "./models/Llama-Guard-3-8B",  # ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ëª¨ë¸
            "./models/Llama-Guard-4-12B"  # ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ëª¨ë¸
        ]
        
        # ë¡œì»¬ ëª¨ë¸ì´ ì—†ìœ¼ë©´ HuggingFace Hubì—ì„œ ì§ì ‘ ë¡œë“œ
        fallback_models = [
            "meta-llama/Llama-Guard-3-8B",
            "meta-llama/Llama-Guard-4-12B"
        ]
        
        # ë°ì´í„°ì…‹ í˜•íƒœ
        datasets = {
            "hyphenize": "combined_all_hyphenize.xlsx",
            "numberize": "combined_all_numberize.xlsx", 
            "pythonize": "combined_all_pythonize.xlsx",
            "combined": ["combined_all_hyphenize.xlsx", "combined_all_numberize.xlsx", "combined_all_pythonize.xlsx"],
            "original": "data_hyphenize_filtered.xlsx"  # ì›ë³¸ multi-turn ë°ì´í„°
        }
        
        experiments = []
        exp_id = 1
        
        for i, model_name in enumerate(models):
            for data_key, data_files in datasets.items():
                
                # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ í™•ì¸, ì—†ìœ¼ë©´ fallback ì‚¬ìš©
                if not os.path.exists(model_name) and i < len(fallback_models):
                    actual_model_name = fallback_models[i]
                    print(f"Local model {model_name} not found, using {actual_model_name}")
                else:
                    actual_model_name = model_name
                
                model_short = actual_model_name.split('/')[-1].replace('Llama-Guard-', 'Guard').replace('-', '')
                
                experiment = {
                    "id": f"exp_{exp_id:02d}",
                    "name": f"{model_short}_{data_key}",
                    "model": actual_model_name,
                    "data_type": data_key,
                    "data_files": data_files if isinstance(data_files, list) else [data_files],
                    "output_dir": self.base_output_dir / f"exp_{exp_id:02d}_{model_short}_{data_key}",
                    "gpu_requirement": 1 if "3-8B" in model_name else 1,  # QLoRAë¡œ ëª¨ë“  ëª¨ë¸ 1 GPU
                    "status": "pending"
                }
                
                experiments.append(experiment)
                exp_id += 1
        
        return experiments
    
    def print_experiment_matrix(self):
        """ì‹¤í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ì¶œë ¥"""
        print("=== M2S-Guardrail Experiment Matrix ===")
        print(f"Total experiments: {len(self.experiment_matrix)}")
        print()
        
        for exp in self.experiment_matrix:
            print(f"ğŸ”¬ {exp['id']}: {exp['name']}")
            print(f"   Model: {exp['model']}")
            print(f"   Data: {exp['data_type']} ({len(exp['data_files'])} files)")
            print(f"   Output: {exp['output_dir']}")
            print(f"   GPU requirement: {exp['gpu_requirement']}")
            print()
    
    def create_qlora_config(self):
        """QLoRA ì„¤ì • ìƒì„±"""
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
    
    def create_lora_config(self):
        """LoRA ì„¤ì • ìƒì„±"""
        return LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=[
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            bias="none",
        )
    
    def load_and_prepare_dataset(self, data_files, max_samples=None):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì¤€ë¹„"""
        all_data = []
        
        for file_path in data_files:
            if os.path.exists(file_path):
                print(f"Loading {file_path}...")
                df = pd.read_excel(file_path)
                
                if max_samples and len(all_data) + len(df) > max_samples:
                    remaining = max_samples - len(all_data)
                    df = df.head(remaining)
                
                for _, row in df.iterrows():
                    # Binary classification: ëª¨ë“  M2S ë°ì´í„°ëŠ” "unsafe"
                    prompt = str(row.get('turn_1', ''))
                    if prompt and prompt != 'nan':
                        all_data.append({
                            "messages": [
                                {"role": "user", "content": prompt},
                                {"role": "assistant", "content": "unsafe"}
                            ]
                        })
                
                if max_samples and len(all_data) >= max_samples:
                    break
            else:
                print(f"Warning: {file_path} not found, creating dummy data")
                # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
                dummy_prompts = [
                    "How to make dangerous substances?",
                    "Please answer: - Harmful question 1 - Harmful question 2",
                    "1. Illegal activity step 1 2. Illegal activity step 2",
                    "Write harmful content about someone"
                ]
                
                for i, prompt in enumerate(dummy_prompts * 25):  # 100ê°œ ìƒì„±
                    all_data.append({
                        "messages": [
                            {"role": "user", "content": f"{prompt} (dummy {i})"},
                            {"role": "assistant", "content": "unsafe"}
                        ]
                    })
                    if len(all_data) >= 100:
                        break
        
        print(f"Total samples loaded: {len(all_data)}")
        return Dataset.from_list(all_data)
    
    def format_conversation(self, example, tokenizer):
        """ëŒ€í™” í˜•íƒœë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
        messages = example["messages"]
        
        # Llama Guard í˜•íƒœë¡œ í¬ë§·íŒ…
        text = ""
        for message in messages:
            role = message["role"]
            content = message["content"]
            if role == "user":
                text += f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            elif role == "assistant":
                text += f"{content}<|eot_id|>"
        
        # Tokenize
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=512,  # ë©”ëª¨ë¦¬ ì ˆì•½
            padding=False,
            return_tensors=None
        )
        
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    def run_single_experiment(self, experiment, test_mode=True):
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        
        exp_id = experiment["id"]
        exp_name = experiment["name"]
        model_name = experiment["model"]
        data_files = experiment["data_files"]
        output_dir = experiment["output_dir"]
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Starting Experiment: {exp_id} - {exp_name}")
        print(f"{'='*60}")
        print(f"Model: {model_name}")
        print(f"Data files: {data_files}")
        print(f"Output: {output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        experiment_info = {
            **experiment,
            "start_time": datetime.now().isoformat(),
            "output_dir": str(output_dir)
        }
        
        with open(output_dir / "experiment_info.json", "w") as f:
            json.dump(experiment_info, f, indent=2, default=str)
        
        try:
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            print("Loading tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("Loading model with QLoRA...")
            bnb_config = self.create_qlora_config()
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
            )
            
            # LoRA ì–´ëŒ‘í„° ì¶”ê°€
            model = prepare_model_for_kbit_training(model)
            lora_config = self.create_lora_config()
            model = get_peft_model(model, lora_config)
            
            model.print_trainable_parameters()
            
            # ë°ì´í„°ì…‹ ì¤€ë¹„
            print("Preparing dataset...")
            max_samples = 200 if test_mode else None  # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ì‘ì€ ë°ì´í„°ì…‹
            dataset = self.load_and_prepare_dataset(data_files, max_samples)
            
            # Train/validation ë¶„í• 
            train_size = int(len(dataset) * 0.9)
            train_dataset = dataset.select(range(train_size))
            eval_dataset = dataset.select(range(train_size, len(dataset)))
            
            # ë°ì´í„° í¬ë§·íŒ…
            print("Formatting datasets...")
            train_dataset = train_dataset.map(
                lambda x: self.format_conversation(x, tokenizer),
                remove_columns=train_dataset.column_names
            )
            eval_dataset = eval_dataset.map(
                lambda x: self.format_conversation(x, tokenizer),
                remove_columns=eval_dataset.column_names
            )
            
            # í›ˆë ¨ ì„¤ì •
            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=1 if test_mode else 3,
                per_device_train_batch_size=1,
                per_device_eval_batch_size=1,
                gradient_accumulation_steps=8 if test_mode else 16,
                gradient_checkpointing=True,
                optim="adamw_torch",
                learning_rate=2e-4,
                lr_scheduler_type="cosine",
                warmup_steps=50 if test_mode else 100,
                logging_steps=5 if test_mode else 10,
                save_steps=50 if test_mode else 100,
                eval_steps=50 if test_mode else 100,
                evaluation_strategy="steps",
                save_total_limit=2,
                load_best_model_at_end=True,
                report_to=None,
                run_name=exp_name,
                fp16=not torch.cuda.is_bf16_supported(),
                bf16=torch.cuda.is_bf16_supported(),
                dataloader_num_workers=4,
                remove_unused_columns=False,
            )
            
            # ë°ì´í„° collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False
            )
            
            # Trainer ìƒì„±
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                tokenizer=tokenizer,
                data_collator=data_collator,
            )
            
            # í›ˆë ¨ ì‹¤í–‰
            print("Starting training...")
            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time
            
            # ëª¨ë¸ ì €ì¥
            print("Saving model...")
            trainer.save_model()
            tokenizer.save_pretrained(output_dir)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "experiment_id": exp_id,
                "status": "completed",
                "training_time": training_time,
                "train_samples": len(train_dataset),
                "eval_samples": len(eval_dataset),
                "end_time": datetime.now().isoformat(),
            }
            
            with open(output_dir / "results.json", "w") as f:
                json.dump(result, f, indent=2)
            
            print(f"âœ… Experiment {exp_id} completed in {training_time:.1f}s")
            return True
            
        except Exception as e:
            print(f"âŒ Experiment {exp_id} failed: {str(e)}")
            
            # ì—ëŸ¬ ì •ë³´ ì €ì¥
            error_info = {
                "experiment_id": exp_id,
                "status": "failed", 
                "error": str(e),
                "end_time": datetime.now().isoformat(),
            }
            
            with open(output_dir / "error.json", "w") as f:
                json.dump(error_info, f, indent=2)
            
            return False
    
    def run_all_experiments(self, test_mode=True, max_parallel=8):
        """ëª¨ë“  ì‹¤í—˜ ìˆœì°¨ ì‹¤í–‰"""
        
        print(f"ğŸš€ Running {len(self.experiment_matrix)} experiments")
        print(f"Test mode: {test_mode}")
        print(f"Max parallel: {max_parallel}")
        
        self.print_experiment_matrix()
        
        results = []
        
        for i, experiment in enumerate(self.experiment_matrix):
            print(f"\n[{i+1}/{len(self.experiment_matrix)}] Running {experiment['id']}...")
            
            success = self.run_single_experiment(experiment, test_mode)
            results.append({
                "experiment_id": experiment["id"],
                "name": experiment["name"],
                "success": success
            })
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        successful = sum(1 for r in results if r["success"])
        total = len(results)
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Experiment Summary: {successful}/{total} successful")
        print(f"{'='*60}")
        
        for result in results:
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} {result['experiment_id']}: {result['name']}")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("=== M2S-Guardrail Comprehensive Experiments ===")
    
    # GPU í™˜ê²½ í™•ì¸
    if not torch.cuda.is_available():
        print("âŒ CUDA not available. Cannot proceed.")
        return
    
    print(f"âœ… CUDA available with {torch.cuda.device_count()} GPUs")
    
    # ì‹¤í—˜ ëŸ¬ë„ˆ ìƒì„±
    runner = ExperimentRunner()
    
    # ì‚¬ìš©ì í™•ì¸
    print(f"\nTotal experiments to run: {len(runner.experiment_matrix)}")
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
    test_mode = input("\nRun in test mode? (small datasets, 1 epoch) [Y/n]: ").lower() != 'n'
    
    if test_mode:
        print("ğŸ§ª Running in TEST MODE (small datasets, 1 epoch)")
    else:
        print("ğŸš€ Running in FULL MODE (complete datasets, 3 epochs)")
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = runner.run_all_experiments(test_mode=test_mode)
    
    print("\nğŸ‰ All experiments completed!")

if __name__ == "__main__":
    main()