#!/bin/bash
# GPU ì„œë²„ìš© M2S-Guardrail ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
# /eic/data í™˜ê²½ì— ë§žì¶° ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ

set -e

echo "=== M2S-Guardrail GPU Server Model Download ==="

# GPU ë””ë ‰í„°ë¦¬ í™•ì¸
if [ -z "$HF_HOME" ]; then
    echo "âŒ HF_HOME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ë¨¼ì € í™˜ê²½ì„ í™œì„±í™”í•˜ì„¸ìš”: source /eic/data/[your_dir]/activate_env.sh"
    exit 1
fi

echo "âœ… HuggingFace cache directory: $HF_HOME"

# ëª¨ë¸ ì €ìž¥ ë””ë ‰í„°ë¦¬ ìƒì„± (í˜„ìž¬ í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ì—)
MODELS_DIR="./models"
mkdir -p $MODELS_DIR

echo "Creating models directory: $MODELS_DIR"

# HuggingFace CLI ê²½ë¡œ í™•ì¸
if command -v huggingface-cli &> /dev/null; then
    echo "âœ… huggingface-cli found at: $(which huggingface-cli)"
else
    echo "âŒ huggingface-cli not found"
    echo "ì„¤ì¹˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”: pip install huggingface-hub"
    exit 1
fi

# Llama Guard 3 (8B) ë‹¤ìš´ë¡œë“œ
echo ""
echo "ðŸ“¥ Downloading Llama Guard 3 (8B)..."
echo "This may take 10-20 minutes depending on your internet speed..."
echo "Models will be cached in: $HF_HOME"

huggingface-cli download meta-llama/Llama-Guard-3-8B \
    --local-dir $MODELS_DIR/Llama-Guard-3-8B \
    --local-dir-use-symlinks False

echo "âœ… Llama Guard 3 (8B) downloaded to: $MODELS_DIR/Llama-Guard-3-8B"

# ë‹¤ìš´ë¡œë“œ í™•ì¸
if [ -d "$MODELS_DIR/Llama-Guard-3-8B" ]; then
    echo "ðŸ“Š Model files:"
    ls -lah $MODELS_DIR/Llama-Guard-3-8B/
    
    # ëª¨ë¸ í¬ê¸° í™•ì¸
    echo ""
    echo "ðŸ“ˆ Total size:"
    du -sh $MODELS_DIR/Llama-Guard-3-8B/
else
    echo "âŒ Download failed"
    exit 1
fi

# Llama Guard 4 (12B) ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ í™•ì¸
echo ""
read -p "Also download Llama Guard 4 (12B)? This will take additional 20-30 minutes [y/N]: " -n 1 -r
echo

if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "ðŸ“¥ Downloading Llama Guard 4 (12B)..."
    
    huggingface-cli download meta-llama/Llama-Guard-4-12B \
        --local-dir $MODELS_DIR/Llama-Guard-4-12B \
        --local-dir-use-symlinks False
    
    echo "âœ… Llama Guard 4 (12B) downloaded to: $MODELS_DIR/Llama-Guard-4-12B"
    
    if [ -d "$MODELS_DIR/Llama-Guard-4-12B" ]; then
        echo "ðŸ“Š Model files:"
        ls -lah $MODELS_DIR/Llama-Guard-4-12B/
        
        echo ""
        echo "ðŸ“ˆ Total size:"
        du -sh $MODELS_DIR/Llama-Guard-4-12B/
    fi
fi

# ì „ì²´ ìš©ëŸ‰ í™•ì¸
echo ""
echo "ðŸ“‹ Download Summary:"
echo "Models directory: $MODELS_DIR"
echo "Total size: $(du -sh $MODELS_DIR 2>/dev/null | cut -f1 || echo 'calculating...')"
echo "Cache directory: $HF_HOME"
echo "Cache size: $(du -sh $HF_HOME 2>/dev/null | cut -f1 || echo 'calculating...')"

echo ""
echo "âœ… Model download completed!"
echo ""
echo "Next steps:"
echo "1. Test environment: python test_environment.py"
echo "2. Run experiments: python train_experiments.py"