#!/usr/bin/env python3
"""
M2S-Guardrail í‰ê°€ìš© ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸
5ê°œ ì†ŒìŠ¤ ë°ì´í„°ë¥¼ ê· ë“±í•˜ê²Œ train/validationìœ¼ë¡œ ë¶„í• 
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from sklearn.model_selection import train_test_split
import os

def load_source_data():
    """5ê°œ ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ"""
    
    # ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤
    data_sources = {
        "MHJ_local": None,  # íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”
        "SafeMTData_1K": None,
        "SafeMTData_Attack600": None,
        "XGuard_Train": None,
        "hh_rlhf": None
    }
    
    # data.xlsxì—ì„œ ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¦¬ (ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
    if os.path.exists("data.xlsx"):
        print("Loading original data.xlsx...")
        df_original = pd.read_excel("data.xlsx")
        
        # Source ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ í™œìš©
        if 'source' in df_original.columns:
            sources = df_original['source'].unique()
            print(f"Found sources: {sources}")
            
            for source in sources:
                source_data = df_original[df_original['source'] == source]
                if 'MHJ' in source or 'local' in source:
                    data_sources["MHJ_local"] = source_data
                elif '1K' in source:
                    data_sources["SafeMTData_1K"] = source_data
                elif 'Attack600' in source or '600' in source:
                    data_sources["SafeMTData_Attack600"] = source_data
        else:
            # source ì»¬ëŸ¼ì´ ì—†ë‹¤ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ì²˜ë¦¬
            print("No source column found, treating as single dataset")
            data_sources["SafeMTData_Original"] = df_original
    
    # ì¶”ê°€ ë°ì´í„° íŒŒì¼ë“¤ í™•ì¸
    additional_files = [
        ("XGuard_Train", "xguard_train.xlsx"),
        ("hh_rlhf", "hh_rlhf.xlsx")
    ]
    
    for source_name, filename in additional_files:
        if os.path.exists(filename):
            print(f"Loading {filename}...")
            data_sources[source_name] = pd.read_excel(filename)
    
    return data_sources

def extract_multi_turn_conversations(df):
    """DataFrameì—ì„œ multi-turn ëŒ€í™” ì¶”ì¶œ"""
    conversations = []
    
    for _, row in df.iterrows():
        # turn_1ë¶€í„° turn_12ê¹Œì§€ í™•ì¸
        turns = []
        for i in range(1, 13):
            turn_col = f'turn_{i}'
            if turn_col in row and pd.notna(row[turn_col]) and str(row[turn_col]).strip():
                turns.append(str(row[turn_col]).strip())
        
        if len(turns) >= 2:  # ìµœì†Œ 2í„´ ì´ìƒì¸ ëŒ€í™”ë§Œ ì„ íƒ
            conversations.append({
                'turns': turns,
                'num_turns': len(turns),
                'original_index': row.name,
                'conversation_text': ' [TURN] '.join(turns)
            })
    
    return conversations

def stratified_split_by_source(data_sources, train_ratio=0.8, random_state=42):
    """ì†ŒìŠ¤ë³„ë¡œ ê· ë“±í•˜ê²Œ train/validation ë¶„í• """
    
    train_data = []
    validation_data = []
    split_info = {}
    
    for source_name, df in data_sources.items():
        if df is None or df.empty:
            print(f"Warning: {source_name} is empty, skipping...")
            continue
        
        print(f"\nProcessing {source_name}: {len(df)} samples")
        
        # Multi-turn ëŒ€í™” ì¶”ì¶œ
        conversations = extract_multi_turn_conversations(df)
        
        if not conversations:
            print(f"Warning: No multi-turn conversations found in {source_name}")
            continue
        
        print(f"Extracted {len(conversations)} multi-turn conversations")
        
        # Train/validation ë¶„í•  (turn lengthë³„ë¡œ stratify)
        turn_lengths = [conv['num_turns'] for conv in conversations]
        
        try:
            train_convs, val_convs = train_test_split(
                conversations,
                train_size=train_ratio,
                stratify=turn_lengths,
                random_state=random_state
            )
        except ValueError:
            # Stratifyê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° (í´ë˜ìŠ¤ê°€ ì ì„ ë•Œ)
            train_convs, val_convs = train_test_split(
                conversations,
                train_size=train_ratio,
                random_state=random_state
            )
        
        train_data.extend(train_convs)
        validation_data.extend(val_convs)
        
        split_info[source_name] = {
            'total': len(conversations),
            'train': len(train_convs),
            'validation': len(val_convs),
            'train_ratio': len(train_convs) / len(conversations)
        }
    
    return train_data, validation_data, split_info

def save_splits(train_data, validation_data, split_info):
    """ë¶„í• ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
    output_dir = Path("evaluation_splits")
    output_dir.mkdir(exist_ok=True)
    
    # Train ë°ì´í„° ì €ì¥
    train_df = pd.DataFrame([
        {
            'conversation_id': i,
            'turns': conv['turns'],
            'num_turns': conv['num_turns'],
            'conversation_text': conv['conversation_text'],
            'original_index': conv['original_index']
        }
        for i, conv in enumerate(train_data)
    ])
    
    # Validation ë°ì´í„° ì €ì¥
    val_df = pd.DataFrame([
        {
            'conversation_id': i,
            'turns': conv['turns'],
            'num_turns': conv['num_turns'],
            'conversation_text': conv['conversation_text'],
            'original_index': conv['original_index']
        }
        for i, conv in enumerate(validation_data)
    ])
    
    # Excel íŒŒì¼ë¡œ ì €ì¥
    train_df.to_excel(output_dir / "train_original_multiturn.xlsx", index=False)
    val_df.to_excel(output_dir / "validation_original_multiturn.xlsx", index=False)
    
    # JSONìœ¼ë¡œë„ ì €ì¥ (ì²˜ë¦¬í•˜ê¸° ì‰½ê²Œ)
    with open(output_dir / "train_conversations.json", 'w', encoding='utf-8') as f:
        json.dump(train_data, f, indent=2, ensure_ascii=False)
    
    with open(output_dir / "validation_conversations.json", 'w', encoding='utf-8') as f:
        json.dump(validation_data, f, indent=2, ensure_ascii=False)
    
    # Split ì •ë³´ ì €ì¥
    with open(output_dir / "split_info.json", 'w', encoding='utf-8') as f:
        json.dump(split_info, f, indent=2)
    
    print(f"\nğŸ“ Files saved to {output_dir}/")
    print(f"   - train_original_multiturn.xlsx ({len(train_data)} conversations)")
    print(f"   - validation_original_multiturn.xlsx ({len(validation_data)} conversations)")
    print(f"   - train_conversations.json")
    print(f"   - validation_conversations.json")
    print(f"   - split_info.json")
    
    return output_dir

def main():
    print("=== M2S-Guardrail Evaluation Data Preparation ===")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("1. Loading source datasets...")
    data_sources = load_source_data()
    
    print(f"Loaded {len([k for k, v in data_sources.items() if v is not None])} datasets:")
    for name, data in data_sources.items():
        if data is not None:
            print(f"   - {name}: {len(data)} samples")
    
    # 2. Train/Validation ë¶„í• 
    print("\n2. Creating train/validation splits...")
    train_data, validation_data, split_info = stratified_split_by_source(data_sources)
    
    # 3. ë¶„í•  ì •ë³´ ì¶œë ¥
    print("\n3. Split Summary:")
    print(f"   Total Train: {len(train_data)} conversations")
    print(f"   Total Validation: {len(validation_data)} conversations")
    
    for source, info in split_info.items():
        print(f"\n   {source}:")
        print(f"     Train: {info['train']}")
        print(f"     Validation: {info['validation']}")
        print(f"     Ratio: {info['train_ratio']:.2f}")
    
    # 4. íŒŒì¼ ì €ì¥
    print("\n4. Saving splits...")
    output_dir = save_splits(train_data, validation_data, split_info)
    
    print("\nâœ… Data preparation completed!")
    print("\nNext steps:")
    print("1. Create M2S training datasets: python create_m2s_training_data.py")
    print("2. Run training experiments: python train_experiments.py")
    print("3. Evaluate models: python evaluate_models.py")

if __name__ == "__main__":
    main()