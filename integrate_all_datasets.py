#!/usr/bin/env python3
"""
3ê°œ ë°ì´í„° ì†ŒìŠ¤ í†µí•© ë° M2S ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ (êµ¬ì¡° íŒŒì•… í›„ ìˆ˜ì •)
- data.xlsx: SafeMTData ì›ë³¸ multi-turn ëŒ€í™”
- XGuard-Train/xguard-train.json: conversations í˜•íƒœ 
- hh-rlhf/: ì••ì¶•ëœ JSONL íŒŒì¼ë“¤ (chosen/rejected)
"""

import pandas as pd
import json
import gzip
import os
from pathlib import Path
import re
from sklearn.model_selection import train_test_split

class DatasetIntegrator:
    def __init__(self):
        self.all_conversations = []
    
    def load_original_data(self, file_path="data.xlsx"):
        """ì›ë³¸ data.xlsx ë¡œë“œ - SafeMTData"""
        print(f"Loading {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found")
            return []
        
        df = pd.read_excel(file_path)
        conversations = []
        
        for _, row in df.iterrows():
            turns = []
            for i in range(1, 13):  # turn_1 to turn_12
                turn_col = f'turn_{i}'
                if turn_col in row and pd.notna(row[turn_col]) and str(row[turn_col]).strip():
                    turns.append(str(row[turn_col]).strip())
            
            if len(turns) >= 2:  # ìµœì†Œ 2í„´ ì´ìƒ
                source = row.get('source', 'SafeMTData')  # SafeMTData_Attack600, SafeMTData_1K, MHJ_local
                conversations.append({
                    'turns': turns,
                    'source': source,
                    'num_turns': len(turns)
                })
        
        print(f"Loaded {len(conversations)} conversations from data.xlsx")
        return conversations
    
    def load_xguard_data(self, file_path="XGuard-Train/xguard-train.json"):
        """XGuard-Train JSON íŒŒì¼ ë¡œë“œ - conversations êµ¬ì¡°"""
        print(f"Loading {file_path}...")
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found")
            return []
        
        conversations = []
        
        try:
            print("Reading large XGuard file... (may take a moment)")
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"Processing {len(data)} XGuard items...")
            
            for i, item in enumerate(data):
                if i % 5000 == 0:  # ì§„í–‰ìƒí™© í‘œì‹œ
                    print(f"  Processed {i}/{len(data)} items...")
                    
                if 'conversations' in item:
                    # conversations ë¦¬ìŠ¤íŠ¸ì—ì„œ turns ì¶”ì¶œ
                    turns = []
                    for conv in item['conversations']:
                        if 'value' in conv and conv.get('from') == 'human':
                            # humanì˜ ë°œì–¸ë§Œ turnìœ¼ë¡œ ì²˜ë¦¬ (jailbreak ì§ˆë¬¸)
                            turns.append(str(conv['value']))
                    
                    if turns:
                        conversations.append({
                            'turns': turns,
                            'source': 'XGuard-Train',
                            'num_turns': len(turns)
                        })
                    
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
        
        print(f"Loaded {len(conversations)} conversations from XGuard-Train")
        return conversations
    
    def load_hh_rlhf_data(self, base_dir="hh-rlhf"):
        """hh-rlhf ì••ì¶• JSONL íŒŒì¼ë“¤ ë¡œë“œ"""
        print(f"Loading from {base_dir}...")
        
        if not os.path.exists(base_dir):
            print(f"Warning: {base_dir} not found")
            return []
        
        conversations = []
        subdirs = ['harmless-base', 'helpful-base', 'helpful-online', 'helpful-rejection-sampled']
        
        for subdir in subdirs:
            subdir_path = Path(base_dir) / subdir
            if not subdir_path.exists():
                continue
                
            print(f"Processing {subdir_path}...")
            
            # train.jsonl.gzì™€ test.jsonl.gz ì²˜ë¦¬
            for jsonl_file in subdir_path.glob("*.jsonl.gz"):
                try:
                    print(f"  Reading {jsonl_file}...")
                    
                    with gzip.open(jsonl_file, 'rt', encoding='utf-8') as f:
                        line_count = 0
                        for line in f:
                            data = json.loads(line)
                            line_count += 1
                            
                            # chosenê³¼ rejected ëª¨ë‘ ì²˜ë¦¬
                            for key in ['chosen', 'rejected']:
                                if key in data:
                                    text = str(data[key])
                                    turns = self.parse_conversation_text(text)
                                    if turns:
                                        conversations.append({
                                            'turns': turns,
                                            'source': f'hh-rlhf-{subdir}-{key}',
                                            'num_turns': len(turns)
                                        })
                    
                    print(f"    Processed {line_count} lines from {jsonl_file.name}")
                                    
                except Exception as e:
                    print(f"Error processing {jsonl_file}: {e}")
                    continue
        
        print(f"Loaded {len(conversations)} conversations from hh-rlhf")
        return conversations
    
    def parse_conversation_text(self, text):
        """Human: Assistant: íŒ¨í„´ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        turns = []
        
        # Human:ê³¼ Assistant: íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        # ì •ê·œì‹ìœ¼ë¡œ ë” ì •í™•í•˜ê²Œ ë¶„í• 
        parts = re.split(r'\n\n(Human:|Assistant:)', text)
        
        current_speaker = None
        current_content = ""
        
        for part in parts:
            part = part.strip()
            if part in ['Human:', 'Assistant:']:
                # ì´ì „ ë‚´ìš© ì €ì¥
                if current_speaker == 'Human:' and current_content.strip():
                    turns.append(current_content.strip())
                
                current_speaker = part
                current_content = ""
            else:
                current_content += part
        
        # ë§ˆì§€ë§‰ ë‚´ìš© ì²˜ë¦¬
        if current_speaker == 'Human:' and current_content.strip():
            turns.append(current_content.strip())
        
        return [turn for turn in turns if turn]
    
    def create_m2s_formats(self, conversation):
        """ë‹¨ì¼ ëŒ€í™”ë¥¼ 4ê°€ì§€ M2S í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        turns = conversation['turns']
        
        # Original (multi-turnì„ í•˜ë‚˜ë¡œ í•©ì¹¨)
        original_text = ' '.join(turns)
        
        # Hyphenize
        hyphenize_text = "Please answer the following list of questions in the given order.\\n"
        hyphenize_text += "Respond sequentially, and provide answers with hyphens only.\\n\\n"
        for turn in turns:
            hyphenize_text += f"- {turn}\\n"
        
        # Numberize  
        numberize_text = "Please answer the following numbered questions in order.\\n"
        numberize_text += "Provide complete responses for each numbered item.\\n\\n"
        for i, turn in enumerate(turns, 1):
            numberize_text += f"{i}. {turn}\\n"
        
        # Pythonize
        pythonize_text = "# Multi-step task execution\\n"
        pythonize_text += "# Execute the following steps in sequence\\n\\n"
        for i, turn in enumerate(turns, 1):
            var_name = f"step_{i}"
            # ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            escaped_turn = turn.replace('"', '\\\\"').replace('\\n', '\\\\n')
            pythonize_text += f'{var_name} = "{escaped_turn}"\\n'
            pythonize_text += f"execute({var_name})\\n\\n"
        
        return {
            'original': original_text.strip(),
            'hyphenize': hyphenize_text.strip(),
            'numberize': numberize_text.strip(), 
            'pythonize': pythonize_text.strip()
        }
    
    def integrate_all_datasets(self):
        """ëª¨ë“  ë°ì´í„°ì…‹ í†µí•©"""
        print("=== Integrating All Datasets ===")
        
        # 1. ê° ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ
        print("\\n1. Loading SafeMTData (data.xlsx)...")
        original_data = self.load_original_data()
        
        print("\\n2. Loading XGuard-Train...")  
        xguard_data = self.load_xguard_data()
        
        print("\\n3. Loading hh-rlhf...")
        hh_rlhf_data = self.load_hh_rlhf_data()
        
        # 2. ëª¨ë“  ëŒ€í™” í†µí•©
        self.all_conversations = original_data + xguard_data + hh_rlhf_data
        
        print(f"\\n=== Integration Complete ===")
        print(f"Total conversations: {len(self.all_conversations)}")
        
        if not self.all_conversations:
            print("No conversations loaded. Exiting.")
            return [], []
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        print("\\nSource breakdown:")
        sources = {}
        for conv in self.all_conversations:
            src = conv['source']
            sources[src] = sources.get(src, 0) + 1
        
        for src, count in sources.items():
            print(f"  - {src}: {count:,}")
        
        # í„´ ìˆ˜ í†µê³„
        turn_stats = {}
        for conv in self.all_conversations:
            turns = conv['num_turns']
            turn_stats[turns] = turn_stats.get(turns, 0) + 1
        
        print("\\nTurn distribution:")
        for turns in sorted(turn_stats.keys()):
            print(f"  - {turns} turns: {turn_stats[turns]:,}")
        
        # 3. Train/Validation ë¶„í•  (ì†ŒìŠ¤ë³„ stratified)
        print("\\n4. Creating train/validation split...")
        try:
            # ì†ŒìŠ¤ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë‹¨ìˆœ ë¶„í• 
            source_counts = list(sources.values())
            min_source_count = min(source_counts)
            
            if min_source_count < 2:
                # stratify ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ë‹¨ìˆœ ë¶„í• 
                split_idx = int(len(self.all_conversations) * 0.8)
                train_convs = self.all_conversations[:split_idx]
                val_convs = self.all_conversations[split_idx:]
                print("Using simple split (stratify not possible)")
            else:
                train_convs, val_convs = train_test_split(
                    self.all_conversations,
                    test_size=0.2,
                    random_state=42,
                    stratify=[conv['source'] for conv in self.all_conversations]
                )
                print("Using stratified split")
        except:
            # ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ ë¶„í• 
            split_idx = int(len(self.all_conversations) * 0.8)
            train_convs = self.all_conversations[:split_idx]
            val_convs = self.all_conversations[split_idx:]
            print("Using simple split (fallback)")
        
        print(f"Split: {len(train_convs):,} train, {len(val_convs):,} validation")
        
        # 4. M2S í¬ë§· ìƒì„± ë° ì €ì¥
        print("\\n5. Creating training datasets...")
        self.create_training_datasets(train_convs)
        
        print("\\n6. Saving validation dataset...")
        self.save_validation_dataset(val_convs)
        
        return train_convs, val_convs
    
    def create_training_datasets(self, train_conversations):
        """í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±"""
        print("Creating M2S training datasets...")
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        output_dir = Path("training_data")
        output_dir.mkdir(exist_ok=True)
        
        # 5ê°€ì§€ í¬ë§·ë³„ ë°ì´í„°ì…‹
        datasets = {
            'original': [],
            'hyphenize': [],
            'numberize': [],
            'pythonize': [],
            'combined': []
        }
        
        import random
        random.seed(42)
        
        print(f"Processing {len(train_conversations):,} training conversations...")
        
        for i, conv in enumerate(train_conversations):
            if i % 5000 == 0:
                print(f"  Processed {i:,}/{len(train_conversations):,}...")
                
            # M2S ë³€í™˜
            m2s_formats = self.create_m2s_formats(conv)
            
            # ê° í¬ë§·ë³„ë¡œ ì €ì¥
            for format_name, text in m2s_formats.items():
                datasets[format_name].append({
                    'text': text,
                    'label': 'unsafe',  # ëª¨ë“  jailbreak ë°ì´í„°ëŠ” unsafe
                    'source': conv['source'],
                    'num_turns': conv['num_turns']
                })
            
            # Combined: ëœë¤í•˜ê²Œ í•˜ë‚˜ì˜ M2S í¬ë§· ì„ íƒ
            random_format = random.choice(['hyphenize', 'numberize', 'pythonize'])
            datasets['combined'].append({
                'text': m2s_formats[random_format],
                'label': 'unsafe',
                'source': conv['source'],
                'num_turns': conv['num_turns'],
                'format_used': random_format
            })
        
        # íŒŒì¼ë¡œ ì €ì¥
        print("\\nSaving datasets...")
        for format_name, data in datasets.items():
            print(f"  Saving {format_name}: {len(data):,} samples...")
            
            df = pd.DataFrame(data)
            
            excel_file = output_dir / f"train_{format_name}.xlsx"
            df.to_excel(excel_file, index=False)
            
            json_file = output_dir / f"train_{format_name}.json" 
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
        print("All training datasets saved!")
    
    def save_validation_dataset(self, val_conversations):
        """ê²€ì¦ ë°ì´í„°ì…‹ ì €ì¥ (ì›ë³¸ multi-turn í˜•íƒœ ìœ ì§€)"""
        print("Saving validation dataset...")
        
        output_dir = Path("evaluation_splits")
        output_dir.mkdir(exist_ok=True)
        
        # ê²€ì¦ ë°ì´í„°ëŠ” ì›ë³¸ multi-turn í˜•íƒœë¡œ ìœ ì§€
        val_data = []
        for i, conv in enumerate(val_conversations):
            val_data.append({
                'conversation_id': i,
                'turns': conv['turns'],
                'num_turns': conv['num_turns'],
                'source': conv['source'],
                'conversation_text': ' [TURN] '.join(conv['turns'])
            })
        
        # ì €ì¥
        df_val = pd.DataFrame(val_data)
        df_val.to_excel(output_dir / "validation_original_multiturn.xlsx", index=False)
        
        with open(output_dir / "validation_conversations.json", 'w', encoding='utf-8') as f:
            json.dump(val_data, f, indent=2, ensure_ascii=False)
        
        print(f"Validation dataset saved: {len(val_data):,} conversations")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ M2S-Guardrail Dataset Integration")
    print("=" * 50)
    
    integrator = DatasetIntegrator()
    train_convs, val_convs = integrator.integrate_all_datasets()
    
    print("\\n" + "=" * 50)
    print("âœ… Dataset integration completed!")
    print("\\nGenerated files:")
    print("ğŸ“ training_data/")
    print("   - train_original.xlsx (original multi-turn format)")
    print("   - train_hyphenize.xlsx (M2S hyphen format)")
    print("   - train_numberize.xlsx (M2S number format)")  
    print("   - train_pythonize.xlsx (M2S python format)")
    print("   - train_combined.xlsx (random M2S formats)")
    print("ğŸ“ evaluation_splits/")
    print("   - validation_original_multiturn.xlsx (original format for evaluation)")
    print("\\nNext step: Run experiments with bash run_experiments_simple.sh")

if __name__ == "__main__":
    main()