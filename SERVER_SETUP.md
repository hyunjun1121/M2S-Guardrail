# ğŸš€ Server Setup Guide

ì„œë²„ì—ì„œ M2S-Guardrail í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ **1ë‹¨ê³„: Repository Clone**

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
cd ~
git clone https://github.com/hyunjun1121/M2S-Guardrail.git
cd M2S-Guardrail

# ìµœì‹  ë²„ì „ í™•ì¸
git log --oneline -3
```

## ğŸ” **2ë‹¨ê³„: GPU í™˜ê²½ ë¶„ì„**

```bash
# í˜„ì¬ GPU ìƒíƒœ í™•ì¸
nvidia-smi

# ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ë¶„ì„
python3 check_gpu_process.py

# ê°„ë‹¨ í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps -ef | grep 697115
cat /proc/697115/cmdline | tr '\0' ' '
```

## ğŸ“¦ **3ë‹¨ê³„: í™˜ê²½ ì„¤ì •**

```bash
# Python í™˜ê²½ í™•ì¸
python3 --version
pip3 --version

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip3 install transformers>=4.36.0
pip3 install datasets>=2.14.0
pip3 install accelerate>=0.24.0
pip3 install peft>=0.6.0
pip3 install bitsandbytes>=0.41.0
pip3 install pandas openpyxl
pip3 install psutil

# CUDA í™˜ê²½ í…ŒìŠ¤íŠ¸
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

## ğŸ“Š **4ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„**

### **ë°©ë²• A: HuggingFaceì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ**
```bash
# HuggingFace CLI ì„¤ì¹˜
pip3 install huggingface_hub

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
huggingface-cli download marslabucla/XGuard-Train --repo-type=dataset --local-dir ./XGuard-Train
huggingface-cli download Anthropic/hh-rlhf --repo-type=dataset --local-dir ./hh-rlhf

# M2S ì „ì²˜ë¦¬ ì‹¤í–‰
python3 m2s_preprocess_new.py
```

### **ë°©ë²• B: ë¡œì»¬ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì „ì†¡ (ê¶Œì¥)**
```bash
# ë¡œì»¬ì—ì„œ ì„œë²„ë¡œ ì „ì†¡ (Windows â†’ Linux)
scp combined_all_*.xlsx hkim@eic-gt-gpu2:~/M2S-Guardrail/
```

## ğŸ¯ **5ë‹¨ê³„: GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**

### **ì‹œë‚˜ë¦¬ì˜¤ A: ì „ì²´ GPU ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°**
```bash
# í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (ì•ˆì „ í™•ì¸ í›„)
# kill 697115

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í™•ì¸
nvidia-smi

# Full SFT ì‹¤í–‰ ê°€ëŠ¥
```

### **ì‹œë‚˜ë¦¬ì˜¤ B: í˜„ì¬ ìƒíƒœ ìœ ì§€**
```bash
# QLoRA ëª¨ë“œë¡œ ì‹¤í–‰
# ê° GPUë‹¹ ~6GB ì‚¬ìš© ê°€ëŠ¥
```

## ğŸš€ **6ë‹¨ê³„: í›ˆë ¨ ì‹œì‘**

### **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
```bash
# ê°„ë‹¨í•œ í™˜ê²½ í…ŒìŠ¤íŠ¸
python3 -c "
import torch
from transformers import AutoTokenizer
print('PyTorch version:', torch.__version__)
print('CUDA available:', torch.cuda.is_available())
print('GPU memory:', torch.cuda.get_device_properties(0).total_memory / 1024**3, 'GB')
"
```

### **ì‹¤ì œ í›ˆë ¨ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)**
```bash
# ë‹¨ê³„ë³„ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì˜ˆì •
# python3 train_llama_guard_binary.py --config guard3_hyphenize
```

## ğŸ”§ **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**

### **CUDA ê´€ë ¨ ì´ìŠˆ**
```bash
# CUDA ë²„ì „ í™•ì¸
nvcc --version
nvidia-smi

# PyTorch CUDA í˜¸í™˜ì„± í™•ì¸
python3 -c "import torch; print(torch.version.cuda)"
```

### **ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**
```bash
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
sudo fuser -v /dev/nvidia*
# ë˜ëŠ”
sudo systemctl restart nvidia-persistenced
```

### **ê¶Œí•œ ì´ìŠˆ**
```bash
# GPU ì ‘ê·¼ ê¶Œí•œ í™•ì¸
ls -la /dev/nvidia*
groups $USER
```

## ğŸ“ **ë‹¤ìŒ ë‹¨ê³„**

1. âœ… Repository clone ì™„ë£Œ
2. âœ… GPU í™˜ê²½ ë¶„ì„ ì™„ë£Œ  
3. â³ í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ì¤€ë¹„
4. â³ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„
5. â³ ì‹¤í—˜ ì‹¤í–‰

---

**í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì²´í¬í•˜ë©° ë‹¨ê³„ë³„ë¡œ ì§„í–‰í•´ì£¼ì„¸ìš”!**