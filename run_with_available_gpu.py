#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import subprocess
import re
import time

def get_gpu_memory_usage():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, check=True)
        
        gpu_info = []
        for line in result.stdout.strip().split('\n'):
            if line.strip():
                parts = line.strip().split(', ')
                gpu_id = int(parts[0])
                used_mb = int(parts[1])
                total_mb = int(parts[2])
                free_mb = total_mb - used_mb
                usage_percent = (used_mb / total_mb) * 100
                
                gpu_info.append({
                    'id': gpu_id,
                    'used_mb': used_mb,
                    'total_mb': total_mb,
                    'free_mb': free_mb,
                    'usage_percent': usage_percent
                })
        
        return gpu_info
    except subprocess.CalledProcessError as e:
        print(f"Error running nvidia-smi: {e}")
        return []

def find_available_gpu(min_free_gb=6):
    """ì‚¬ìš© ê°€ëŠ¥í•œ GPU ì°¾ê¸° (ìµœì†Œ free_gb ì´ìƒ ì—¬ìœ  ë©”ëª¨ë¦¬)"""
    gpu_info = get_gpu_memory_usage()
    
    if not gpu_info:
        return None
    
    print("=== GPU ë©”ëª¨ë¦¬ ì‚¬ìš© í˜„í™© ===")
    for gpu in gpu_info:
        print(f"GPU {gpu['id']}: {gpu['used_mb']:,} MB / {gpu['total_mb']:,} MB ì‚¬ìš© ({gpu['usage_percent']:.1f}%) - ì—¬ìœ : {gpu['free_mb']:,} MB")
    
    # ìµœì†Œ ìš”êµ¬ ë©”ëª¨ë¦¬ ì´ìƒì˜ ì—¬ìœ ê°€ ìˆëŠ” GPU ì°¾ê¸°
    min_free_mb = min_free_gb * 1024
    available_gpus = [gpu for gpu in gpu_info if gpu['free_mb'] >= min_free_mb]
    
    if available_gpus:
        # ê°€ì¥ ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ ë§ì€ GPU ì„ íƒ
        best_gpu = max(available_gpus, key=lambda x: x['free_mb'])
        print(f"\nâœ… GPU {best_gpu['id']} ì„ íƒ (ì—¬ìœ  ë©”ëª¨ë¦¬: {best_gpu['free_mb']:,} MB)")
        return best_gpu['id']
    else:
        print(f"\nâŒ {min_free_gb}GB ì´ìƒ ì—¬ìœ  ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ GPUê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

def run_experiment_on_available_gpu():
    """ì‚¬ìš© ê°€ëŠ¥í•œ GPUì—ì„œ ì‹¤í—˜ ì‹¤í–‰"""
    
    print("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê²€ìƒ‰ ì¤‘...")
    
    # 6GB ì´ìƒ ì—¬ìœ  ë©”ëª¨ë¦¬ê°€ ìˆëŠ” GPU ì°¾ê¸°
    available_gpu = find_available_gpu(min_free_gb=6)
    
    if available_gpu is None:
        print("ëŒ€ê¸° ì¤‘... 5ë¶„ í›„ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.")
        time.sleep(300)  # 5ë¶„ ëŒ€ê¸°
        available_gpu = find_available_gpu(min_free_gb=6)
        
        if available_gpu is None:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
            response = input().lower()
            if response == 'y':
                available_gpu = ""  # CPU ëª¨ë“œ
            else:
                print("ì‹¤í—˜ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    if available_gpu == "":
        print("ğŸ–¥ï¸  CPUì—ì„œ ì‹¤í—˜ ì‹œì‘...")
        os.environ['CUDA_VISIBLE_DEVICES'] = ""
    else:
        print(f"ğŸš€ GPU {available_gpu}ì—ì„œ ì‹¤í—˜ ì‹œì‘...")
        os.environ['CUDA_VISIBLE_DEVICES'] = str(available_gpu)
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiments = [
        {
            "name": "integrated_data",
            "train": "integrated_data_train.xlsx",
            "val": "integrated_data_val.xlsx"
        },
        {
            "name": "hyphenize",
            "train": "m2s_hyphenize_data_train.xlsx", 
            "val": "m2s_hyphenize_data_val.xlsx"
        },
        {
            "name": "numberize",
            "train": "m2s_numberize_data_train.xlsx",
            "val": "m2s_numberize_data_val.xlsx"  
        },
        {
            "name": "pythonize",
            "train": "m2s_pythonize_data_train.xlsx",
            "val": "m2s_pythonize_data_val.xlsx"
        },
        {
            "name": "combined_all", 
            "train": "m2s_combined_all_data_train.xlsx",
            "val": "m2s_combined_all_data_val.xlsx"
        }
    ]
    
    for i, exp in enumerate(experiments, 1):
        print(f"\nğŸ§ª ì‹¤í—˜ {i}/5: {exp['name']} ì‹œì‘...")
        
        cmd = [
            "python", "run_guardrail_with_validation_fixed.py",
            "--model", "Llama-Prompt-Guard-2-86M",
            "--train", exp['train'],
            "--val", exp['val'], 
            "--name", exp['name']
        ]
        
        try:
            print(f"ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
            result = subprocess.run(cmd, check=True, capture_output=False)
            print(f"âœ… ì‹¤í—˜ {exp['name']} ì™„ë£Œ!")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ì‹¤í—˜ {exp['name']} ì‹¤íŒ¨: {e}")
            print("ë‹¤ìŒ ì‹¤í—˜ìœ¼ë¡œ ê³„ì† ì§„í–‰...")
            continue
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        
        # ì‹¤í—˜ ê°„ 5ë¶„ ëŒ€ê¸° (ë©”ëª¨ë¦¬ ì •ë¦¬)
        if i < len(experiments):
            print("ğŸ’¤ ë‹¤ìŒ ì‹¤í—˜ì„ ìœ„í•´ 5ë¶„ ëŒ€ê¸° ì¤‘...")
            time.sleep(300)
    
    print("\nğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    run_experiment_on_available_gpu()