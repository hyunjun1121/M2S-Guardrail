#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification, 
    AutoModelForCausalLM, TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, DataCollatorWithPadding
)
from datasets import Dataset
import argparse
import json

def format_prompt_guard_data(turns_list):
    """Prompt-Guard í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ… (injection íƒì§€ìš©)"""
    prompts_and_labels = []
    
    # ê° í„´ì„ prompt injectionìœ¼ë¡œ ë¶„ë¥˜
    for turn in turns_list:
        if turn and str(turn).strip():
            prompts_and_labels.append({
                'text': str(turn).strip(),
                'label': 1  # 1 = injection/jailbreak
            })
    
    return prompts_and_labels

def process_harmful_dataset(data_file, model_type):
    """Harmful ë°ì´í„°ì…‹ì„ Guardrail í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_file}")
    df = pd.read_excel(data_file)
    
    all_training_data = []
    
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"Processing {idx}/{len(df)} rows...")
        
        turns = []
        
        # M2S ì••ì¶•ëœ ë°ì´í„°ì¸ ê²½ìš°
        if 'prompt' in row and pd.notna(row['prompt']):
            # M2S ì „ì²´ í”„ë¡¬í”„íŠ¸ë¥¼ harmfulë¡œ ì²˜ë¦¬
            if model_type == "prompt_guard":
                all_training_data.append({
                    'text': str(row['prompt']).strip(),
                    'label': 1
                })
        
        # ì›ë³¸ í„´ ë°ì´í„°ì¸ ê²½ìš°
        else:
            # turn_1ë¶€í„° turn_12ê¹Œì§€ ì¶”ì¶œ
            for i in range(1, 13):
                turn_col = f'turn_{i}'
                if turn_col in row and pd.notna(row[turn_col]):
                    turns.append(str(row[turn_col]).strip())
            
            if turns and model_type == "prompt_guard":
                formatted_prompts = format_prompt_guard_data(turns)
                all_training_data.extend(formatted_prompts)
    
    print(f"ì´ í›ˆë ¨ ìƒ˜í”Œ ìˆ˜: {len(all_training_data)}")
    return all_training_data

def create_prompt_guard_dataset(training_data, tokenizer, max_length=512):
    """Prompt-Guardìš© ë°ì´í„°ì…‹ ìƒì„± (ì„±ê³µí•œ ë°©ì‹ ì‚¬ìš©)"""
    texts = [item['text'] for item in training_data]
    labels = [item['label'] for item in training_data]
    
    print(f"ì²« 5ê°œ í…ìŠ¤íŠ¸ ìƒ˜í”Œ:")
    for i, text in enumerate(texts[:5]):
        print(f"  {i+1}. {text[:100]}... (label: {labels[i]})")
    
    def tokenize_function(examples):
        result = tokenizer(
            examples['text'],
            truncation=True,
            padding=True,  # ì„±ê³µí•œ ì„¤ì •
            max_length=max_length,
            return_tensors=None  # Datasetì—ì„œëŠ” None ì‚¬ìš©
        )
        return result
    
    dataset = Dataset.from_dict({"text": texts, "labels": labels})
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    print("í† í°í™” ì™„ë£Œ")
    
    return tokenized_dataset

def run_guardrail_final(model_name, train_file, val_file, experiment_name):
    """ìµœì¢… Guardrail Fine-tuning (ì„±ê³µí•œ ë°©ì‹ ê¸°ë°˜)"""
    
    print(f"ğŸš€ Guardrail Fine-tuning ì‹œì‘ (ìµœì¢… ë²„ì „)")
    print(f"ëª¨ë¸: {model_name}")
    print(f"Train ë°ì´í„°: {train_file}")
    print(f"Validation ë°ì´í„°: {val_file}")
    print(f"ì‹¤í—˜: {experiment_name}")
    
    # í˜„ì¬ëŠ” Prompt-Guardë§Œ ì§€ì›
    if "Prompt-Guard" not in model_name:
        print("âŒ í˜„ì¬ëŠ” Llama-Prompt-Guard-2-86Më§Œ ì§€ì›ë©ë‹ˆë‹¤.")
        return None
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "./models/Llama-Prompt-Guard-2-86M"
    model_type = "prompt_guard"
    max_length = 512
    
    print(f"ëª¨ë¸ íƒ€ì…: {model_type}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ (ì„±ê³µí•œ ì„¤ì • ì‚¬ìš©)
    print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path,
        torch_dtype=torch.float32,  # ì„±ê³µí•œ ì„¤ì •
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„°: {model.num_parameters():,}")
    
    # Train ë°ì´í„° ì²˜ë¦¬
    print("Train ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
    train_data = process_harmful_dataset(train_file, model_type)
    
    # Validation ë°ì´í„° ì²˜ë¦¬
    val_data = None
    if val_file and os.path.exists(val_file):
        print("Validation ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        val_data = process_harmful_dataset(val_file, model_type)
    
    # ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ ì ˆì•½)
    train_sample_size = min(2000, len(train_data))  # ë” ì‘ê²Œ ì„¤ì •
    if len(train_data) > train_sample_size:
        import random
        train_data = random.sample(train_data, train_sample_size)
        print(f"Train ìƒ˜í”Œë§: {train_sample_size}ê°œ ì‚¬ìš©")
    
    if val_data:
        val_sample_size = min(500, len(val_data))  # ë” ì‘ê²Œ ì„¤ì •
        if len(val_data) > val_sample_size:
            import random
            val_data = random.sample(val_data, val_sample_size)
            print(f"Validation ìƒ˜í”Œë§: {val_sample_size}ê°œ ì‚¬ìš©")
    
    # ë°ì´í„°ì…‹ ìƒì„± (ì„±ê³µí•œ ë°©ì‹ ì‚¬ìš©)
    train_dataset = create_prompt_guard_dataset(train_data, tokenizer, max_length)
    val_dataset = create_prompt_guard_dataset(val_data, tokenizer, max_length) if val_data else None
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬
    output_dir = f"./fine_tuned_models/{model_name.replace('/', '_')}_{experiment_name}"
    os.makedirs(output_dir, exist_ok=True)
    
    # í›ˆë ¨ ì„¤ì • (ì„±ê³µí•œ ì„¤ì • ì‚¬ìš©)
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=2,  # ì§§ê²Œ ì„¤ì •
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=2,
        learning_rate=2e-5,
        weight_decay=0.01,
        logging_steps=50,
        save_steps=200,
        eval_steps=200,
        save_strategy="steps",
        eval_strategy="steps" if val_dataset else "no",
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss" if val_dataset else None,
        fp16=False,  # ì„±ê³µí•œ ì„¤ì •
        bf16=False,  # ì„±ê³µí•œ ì„¤ì •
        gradient_checkpointing=False,  # ê°„ì†Œí™”
        dataloader_pin_memory=False,
        remove_unused_columns=True,
        report_to=None,
    )
    
    # Trainer ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ”¥ Fine-tuning ì‹œì‘...")
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    # í›ˆë ¨ í†µê³„ ì €ì¥
    stats = {
        "model_name": model_name,
        "model_type": model_type,
        "experiment_name": experiment_name,
        "train_file": train_file,
        "val_file": val_file,
        "train_samples": len(train_data),
        "val_samples": len(val_data) if val_data else 0,
        "output_dir": output_dir,
        "max_length": max_length,
        "model_parameters": model.num_parameters()
    }
    
    with open(os.path.join(output_dir, "training_stats.json"), 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"âœ… Fine-tuning ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ“Š Train ìƒ˜í”Œ: {len(train_data)}")
    print(f"ğŸ“Š Validation ìƒ˜í”Œ: {len(val_data) if val_data else 0}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del trainer, model
    torch.cuda.empty_cache()
    
    return output_dir

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Final Guardrail Model Fine-tuning')
    parser.add_argument('--model', required=True, 
                       choices=['Llama-Guard-4-12B', 'Llama-Prompt-Guard-2-86M'],
                       help='Guardrail model to fine-tune')
    parser.add_argument('--train', required=True, 
                       help='Path to training dataset (Excel file)')
    parser.add_argument('--val', required=False,
                       help='Path to validation dataset (Excel file)')
    parser.add_argument('--name', required=True,
                       help='Experiment name')
    
    args = parser.parse_args()
    
    try:
        result_dir = run_guardrail_final(args.model, args.train, args.val, args.name)
        print(f"\nğŸ‰ ì‹¤í—˜ ì„±ê³µ! ê²°ê³¼: {result_dir}")
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()